{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# Read files into dataframe\n",
    "train_df = pd.read_csv(\"./data/health-diagnostics-train.csv\", na_values=[\"#NULL!\"])\n",
    "test_df = pd.read_csv(\"./data/health-diagnostics-test.csv\", na_values=[\"#NULL!\"])\n",
    "\n",
    "# Change nulls to arbitary number and change dtype to integers\n",
    "def replace_nulls(df):\n",
    "    for f in df.columns:\n",
    "        if df[f].dtype != \"int\":\n",
    "            df[f].fillna(-999, inplace=True)    # xgboost can handle missing values\n",
    "            df[f] = df[f].astype(int)\n",
    "\n",
    "replace_nulls(test_df)\n",
    "replace_nulls(train_df)\n",
    "            \n",
    "# Concat train and test with keys and create dummies\n",
    "temp_df = pd.get_dummies(pd.concat([train_df.iloc[:,:-1], test_df], keys=[0,1], sort=False), \n",
    "                         columns=['env', 'lifestyle'], drop_first=True)\n",
    "\n",
    "# Selecting data from multi index \n",
    "train_dummies, test_dummies = temp_df.xs(0).copy(), temp_df.xs(1).copy()\n",
    "\n",
    "# Add target to end of train dummies df\n",
    "train_dummies['target'] = train_df['target']\n",
    "\n",
    "# Split features and target into X and y\n",
    "X, y = train_dummies.iloc[:,:-1], train_dummies.iloc[:,-1]\n",
    "\n",
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Convert train_df into Dmatrix\n",
    "train_set_dmatrix = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Convert X_train and y_train into Dmatrix\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Scale pos weight is sum of neg divided by sum of pos\n",
    "weight = (sum(train_df['target'] == 0)) / (sum(train_df['target'] == 1))\n",
    "\n",
    "# # Setting kfolds for cv\n",
    "kfold = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameters - find max depth and min child weight first as they have higher impact\n",
    "cv_params = {\"max_depth\": [5, 6, 7],            # num of nodes from root to farthest leaf (~3-10)\n",
    "             \"min_child_weight\": [1, 2, 3]}     # min sum of weights in order to create new node\n",
    "ind_params = {\"learning_rate\": 0.01,            # step size shrinkage to prevent overfitting (~0-1)\n",
    "              \"n_estimators\": 90,               # number of trees\n",
    "              \"seed\": 42,                       # for reproducibility\n",
    "              \"subsample\": 0.6,                 # percentage of samples per tree (~0.5-9)\n",
    "              \"colsample_bytree\": 0.6,          # percerntage of features per tree (~0.5-9)\n",
    "              \"objective\": \"binary:logistic\",   # returns predicted probability\n",
    "              \"scale_pos_weight\": weight,       # high class imbalance\n",
    "              \"missing\": -999}                  # handle missing values\n",
    "\n",
    "# GridSearch evaluates a model with varying parameters to find the best possible combination\n",
    "opt_gbm = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                       cv_params, \n",
    "                       scoring=\"f1\",            # for imbalanced data & very few positives\n",
    "                       cv=kfold, \n",
    "                       n_jobs=-1)               # as many threads available in parallel\n",
    "\n",
    "opt_gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = opt_gbm.predict(X_test)\n",
    "\n",
    "opt_gbm.best_params_                            # finding params for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - find learning rate and subsample next\n",
    "cv_params = {\"learning_rate\": [0.01, 0.001], \n",
    "             \"subsample\": [0.5, 0.6, 0.7], \n",
    "             \"colsample_bytree\": [0.5, 0.6, 0.7]}\n",
    "ind_params = {\"n_estimators\": 90, \n",
    "              \"seed\": 42, \n",
    "              \"objective\": \"binary:logistic\", \n",
    "              \"max_depth\": 6,                   # chosen as best param\n",
    "              \"min_child_weight\": 1,            # chosen as best param\n",
    "              \"scale_pos_weight\": weight,\n",
    "              \"missing\": -999}\n",
    "\n",
    "opt_gbm = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                       cv_params, \n",
    "                       scoring=\"f1\", \n",
    "                       cv=kfold, \n",
    "                       n_jobs=-1)\n",
    "\n",
    "opt_gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = opt_gbm.predict(X_test)\n",
    "\n",
    "opt_gbm.best_params_                            # finding params for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of params we have determined\n",
    "our_params = {\"eta\": 0.001,                     # chosen as best param - aka learning rate\n",
    "              \"seed\": 42, \n",
    "              \"subsample\": 0.5,                 # chosen as best param\n",
    "              \"colsample_bytree\": 0.6,          # chosen as best param\n",
    "              \"objective\": \"binary:logistic\", \n",
    "              \"max_depth\": 6, \n",
    "              \"min_child_weight\": 1, \n",
    "              \"scale_pos_weight\": weight,\n",
    "              \"missing\": -999}\n",
    "\n",
    "# CV estimates the preformance of one set of parameter on unseen data\n",
    "cv_xgb = xgb.cv(our_params, \n",
    "                train_dmatrix, \n",
    "                num_boost_round=300,            # number of trees - aka n_estimators\n",
    "                nfold=12,\n",
    "                metrics=[\"error\"],              # binary class error rate (0.5 threshold)\n",
    "                stratified=True,\n",
    "                early_stopping_rounds=30)       # finish early if it doesn't improve\n",
    "\n",
    "# Determine final boost round\n",
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_xgb = xgb.XGBClassifier(learning_rate=0.001, \n",
    "                           seed=42, \n",
    "                           subsample=0.5, \n",
    "                           colsample_bytree=0.6, \n",
    "                           objective=\"binary:logistic\", \n",
    "                           max_depth=6, \n",
    "                           min_child_weight=1, \n",
    "                           scale_pos_weight=weight, \n",
    "                           n_estimators=115,      # taken from last round in prev step\n",
    "                           missing=-999)\n",
    "\n",
    "cl_xgb.fit(X_train, y_train)\n",
    "y_pred = cl_xgb.predict(X_test)\n",
    "\n",
    "# Let's see how these parameters perform against OOS data using a confusion matrix\n",
    "display(pd.crosstab(y_test, y_pred, rownames=[\"True\"], colnames=[\"Predicted\"], margins=True))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = float(np.sum(y_pred==y_test)) / y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy*100))\n",
    "\n",
    "current_xgb = xgb.train(our_params, \n",
    "                        train_dmatrix,\n",
    "                        num_boost_round=115)\n",
    "\n",
    "# Plot feature importance on current model\n",
    "xgb.plot_importance(current_xgb)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# # Plot decision tree\n",
    "# fig, ax = plt.subplots(figsize=(30, 30))\n",
    "# xgb.plot_tree(final_xgb, num_trees=4, ax=ax)\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(30, 30)\n",
    "# fig.savefig('tree.png')\n",
    "# plt.savefig(\"temp.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our final prediction, we will train our model on the entire train dataset\n",
    "final_xgb = xgb.train(our_params, \n",
    "                      train_set_dmatrix,\n",
    "                      num_boost_round=115)\n",
    "\n",
    "# Convert dataframe into Dmatrix\n",
    "test_set_dmatrix = xgb.DMatrix(test_dummies)\n",
    "\n",
    "# Using out test dataset for prediction\n",
    "y_pred = final_xgb.predict(test_set_dmatrix)\n",
    "\n",
    "# Threshold for converting probability\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Save results to csv for submission\n",
    "results_df = pd.DataFrame({\"index\": test_df.index, \"target\": y_pred})\n",
    "results_df.to_csv(\"./data/xgboost_feat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
